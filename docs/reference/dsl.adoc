== The rule DSL

The rule of a production is specified using a DSL built on top of C++ operator overloading.
Everything of the DSL is defined in the namespace `lexy::dsl::*` and every header available under `lexy/dsl/*`.
The umbrella header `lexy/dsl.hpp` includes all DSL headers.

A `Rule` is an object that defines a specific set of input to be parsed.
It first tries to match a set of characters from the input by comparing the character at the current reader position to the set of expected characters,
temporarily advancing the reader further if necessary.
If the matching was successful, a subset of matched characters are consumed by advancing the reader permanently.
The rule can then produce zero or more values, which are eventually forwarded to the value callback of its production.
If the matching was not successful, an error is produced instead.
A failed rule does not consume any characters.

A `Branch` is a rule that has an associated condition.
The parsing algorithm can efficiently check whether the condition would match at the current reader position.
As such, they are used whenever the algorithm needs to decide between multiple alternatives.
Once the branch condition matches, the branch is taken without any additional backtracking.

A `Token` is a special `Rule` that is an atomic element of the input.
As a rule, it does not produce any value.
Every `Token` is also a `Branch` that uses itself as the condition.

=== Whitespace

[discrete]
==== `lexy::dsl::whitespaced`

.`lexy/whitespace.hpp`
----
whitespaced(rule, whitespace) : Rule
----

By default, no whitespace skipping is done.
Use `whitespaced()` to skip arbitrary long whitespace before parsing a rule.

[horizontal]
Requires::
  `whitespace` is a branch that does not produce any values
Equivalent to::
  `token(while_(whitespace)) + rule`

[godbolt,cpp,id=r5b8Gb]
----
// Skips whitespace and then parses `abc`.
dsl::whitespaced(LEXY_LIT("abc"), dsl::ascii::space)
----

TIP: "whitespace" can mean literal space characters but also things like comments.

[discrete]
==== `Rule::operator[]`

----
rule[whitespace]    : Rule
token[whitespace]   : Token
branch[whitespace]  : Branch
----

Some rules also provide an `operator[]` that does the same as `whitespaced()`.
They are here described as whitespace aware.

[horizontal]
Requires::
  `whitespace` is a branch that does not produce any values
Equivalent to::
  `whitespaced(rule, whitespace)`

[godbolt,cpp,id=sqdze6]
----
// Skips whitespace and then parses `abc`.
LEXY_LIT("abc")[dsl::ascii::space]
----

=== Primitive Tokens

[discrete]
==== `lexy::dsl::any`

.`lexy/dsl/any.hpp`
----
any : Token
----

The `any` token matches anything, i.e. all the remaining input.

[horizontal]
Matches::
  All the remaining input.
Error::
  n/a (it never fails)

NOTE: `any` is useful in combination with partial inputs such as the minus rule or `switch_`.

[discrete]
==== `lexy::dsl::lit`

.`lexy/dsl/literal.hpp`
----
lit_c<C> : Token
lit<Str> : Token

LEXY_LIT(Str) : Token
----

The literal tokens match the specified sequence of characters.
They are whitespace aware.

[horizontal]
Requires::
  * `C` is a character literal.
  * `Str` is a string literal.
+
In both cases, their encoding must be ASCII or match the encoding of the input.

Matches::
  The specified character or string of characters, which are consumed.
Error::
  `lexy::expected_literal` giving it the string and the index where the match failure occurred.

NOTE: `lit<Str>` requires C++20 support for extended NTTPs.
Use the `LEXY_LIT(Str)` macro if your compiler does not support them.

.`lexy/dsl/punctuator.hpp`
----
period    : Token = lit<".">
comma     : Token = lit<",">
colon     : Token = lit<":">
semicolon : Token = lit<";">

hyphen     : Token = lit<"-">
slash      : Token = lit<"/">
backslash  : Token = lit<"\\">
apostrophe : Token = lit<"'">

hash_sign   : Token = lit<"#">
dollar_sign : Token = lit<"$">
at_sign     : Token = lit<"@">
----

The header `lexy/dsl/punctuator.hpp` defines common punctuator literals.
They are equivalent to a literal matching the specified character.
They are whitespace aware.

=== Character classes

[discrete]
==== `lexy::dsl::eof`

.`lexy/dsl/eof.hpp`
----
eof : Token
----

The `eof` token matches EOF.
It is whitespace aware.

[horizontal]
Matches::
  Only if the reader is at the end of the input. It does not consume anything (it can't).
Error::
  `lexy::expected_char_class` with the name `EOF`.

[discrete]
==== `lexy::dsl::newline`

.`lexy/dsl/newline.hpp`
----
newline : Token
----

The `newline` token matches a newline.
It is whitespace aware.

[horizontal]
Matches::
   `\n` or `\r\n`, which is consumed.
Error::
  `lexy::expected_char_class` with the name `newline`.

[discrete]
==== `lexy::dsl::eol`

.`lexy/dsl/newline.hpp`
----
eol : Token
----

The `eol` token matches an end-of-line (EOL).
It is whitespace aware.

[horizontal]
Matches::
  `\n` or `\r\n`, which is consumed.
  Also matches EOF, which is not consumed.
Error::
  `lexy::expected_char_class` with the name `EOL`.

[discrete]
==== `lexy::dsl::ascii::*`

.`lexy/dsl/ascii.hpp`
----
namespace ascii
{
    control : Token // 0x00-0x1F, 0x7F

    blank       : Token // ' ' (space character) or '\t'
    newline     : Token // '\n' or '\r'
    other_space : Token // '\f' or '\v'
    space       : Token // `blank` or `newline` or `other_space`

    lower : Token // a-z
    upper : Token // A-Z
    alpha : Token // `lower` or `upper`

    digit : Token // 0-9
    alnum : Token // `digit` or `alpha`

    punct : Token // One of: !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~

    graph : Token // `alnum` or `punct`
    print : Token // `graph` or ' ' (space characters)

    character : Token // 0x00-0x7F
}
----

All tokens defined in `lexy::dsl::ascii` match one of the categories of ASCII characters.

[horizontal]
Matches::
  Matches and consumes one of the set of ASCII characters indicated in the comments.
Errors::
  A `lexy::expected_char_class` error with name `ASCII.<token>`, where `<token>` is the name of the token.

NOTE: Every ASCII character except for the space character is in exactly one of `control`, `lower`, `upper`, `digit` or `punct`.

[discrete]
==== `lexy::dsl::code_point`

.`lexy/dsl/code_point.hpp`
----
code_point : Token

code_point.capture() : Rule
----

The `code_point` token will match and consume a well-formed Unicode code point according to the encoding of the input.
If `code_point.capture()` is used, the consumed code point will be produced as value.

[horizontal]
Requires::
  The encoding of the input is `lexy::ascii_encoding`, `lexy::utf8_encoding`, `lexy::utf16_encoding`, or `lexy::utf32_encoding`.
Matches::
  Matches and consumes all code units of the next code point.
  For ASCII and UTF-32 this is only one, but for UTF-8 and UTF-16 it can be multiple code units.
  If the code point is too big or a UTF-16 surrogate, it fails.
  For UTF-8, it also fails for overlong sequences.
Values::
  If `.capture()` was called, it will produce the matched code point as a `lexy::code_point`.
Errors::
  If it could not match a valid code point, it fails with a `lexy::expected_char_class` error with name `<encoding>.code_point`.

[godbolt,cpp,id=YYTfoe]
----
// Match and capture one arbitrary code point.
dsl::code_point.capture()
----

TIP: If you want to match a specific code point, use a literal rule instead.
This rule is useful for matching things like string literals that can contain arbitrary code points.

[discrete]
==== `lexy::dsl::operator-`

.`lexy/dsl/minus.hpp`
----
token - except : Token
----

The minus rule matches the given token, but only if `except` does not match on the input the rule has consumed.

[horizontal]
Requires::
  `except` is a token.
Matches::
  Matches and consumes whatever `token` match and consume.
  Then matches `except` on the same input.
  Matching fails if `except` matches the entire input consumed by the token.
Errors::
  Whatever errors are raised if `token` is not matched.
  A generic error with tag `lexy::minus_failure` if `except` has matched.

TIP: Use a minus rule to exclude characters from a character class; e.g. `lexy::dsl::code_point - lexy::dsl::ascii::control` matches all code points except control characters.

NOTE: Minus rules can be chained. This is equivalent to specifying an alternative for `except`.

WARNING: `except` has to match _everything_ the rule has consumed before; partial matches don't count.
Use `token - (except + lexy::dsl::any)` if you want to allow a partial match.

[discrete]
==== `lexy::dsl::token`

.`lexy/dsl/token.hpp`
----
token(rule) : Token
----

The `token` rule turns an arbitrary rule into a token by parsing it and discarding all values it has produced.
It is whitespace aware.

[horizontal]
Matches::
  Whatever `rule` matches, which will be consumed.
Error::
  A generic error with tag `lexy::missing_token` if the `rule` did not match.

NOTE: While `token()` is optimized to prevent any overhead created by constructing values that are later discarded,
it still should only be used when required.

=== Values

The following rules are used to produce additional values without any additional matching.

[discrete]
==== `lexy::dsl::value_*`

.`lexy/dsl/value.hpp`
----
value_c<Value> : Rule
value_f<Fn>    : Rule
value_t<T>     : Rule
value_str<Str> : Rule

LEXY_VALUE_STR(Str) : Rule
----

The `value_*` rules create a constant value without parsing anything.

[horizontal]
Requires::
  * `Value` is any constant.
  * `Fn` is a pointer to a function taking no arguments.
  * `T` is a default-constructible type.
  * `Str` is a string literal.
Matches::
  Any input, but does not consume anything.
Value::
  `value_c`::: The specified constant.
  `value_f`::: The result of invoking the function.
  `value_t`::: A default constructed object of the specified type.
  `value_str`::: The string literal as a pointer, followed by its size.
Error::
  n/a (it does not fail)

TIP: Use the `value_*` rules only to create symmetry between different branches.
Everything they do, can also be achieved using callbacks, which is usually a better solution.

WARNING: The function might not be called or the object might not be constructed in all situations. You cannot rely on their side effects.

NOTE: `value_str<Str>` requires C++20 support for extended NTTPs.
Use the `LEXY_VALUE_STR(Str)` macro if your compiler does not support them.

[discrete]
==== `lexy::dsl::nullopt`

.`lexy/dsl/option.hpp`
[source,cpp]
----
namespace lexy
{
    struct nullopt
    {
        template <typename T>
        constexpr operator T() const
        {
            return T();
        }
    };
}
----

.`lexy/dsl/option.hpp`
----
nullopt : Rule
----

The `nullopt` rule produces a value of type `lexy::nullopt` without parsing anything.

[horizontal]
Matches::
  Any input, but does not consume anything.
Value::
  An object of type `lexy::nullopt`, which is convertible to any other type.
Error::
  n/a (it does not fail)

NOTE: It is meant to be used for symmetry with together with the `opt()` rule.

[discrete]
==== `lexy::dsl::label` and `lexy::dsl::id`

.`lexy/dsl/label.hpp`
[source,cpp]
----
namespace lexy
{
    template <typename Tag>
    struct label
    {
        // only if Tag::value is well-formed
        consteval operator auto() const
        {
            return Tag::value;
        }
    };

    template <auto Id>
    using id = label<std::integral_constant<int, Id>>;
}
----

.`lexy/dsl/label.hpp`
----
label<Tag> : Rule
id<Id>     : Rule
----

The `label` and `id` rules are used to disambiguate between two branches that create otherwise the same values but should resolve to different callbacks.
They simply produce the empty tag object or the id to differentiate them without parsing anything.

[horizontal]
Requires::
  * `Tag` is any type.
  * `Id` is an integer constant.
Matches::
  Any input, but does not consume anything.
Value::
  `label<Tag>`::: A `lexy::label<Tag>` object.
  `id<Id>`::: A `lexy::id<Id>` object.
Error::
  n/a (it does not fail)

.`lexy/dsl/label.hpp`
----
label<Tag>(rule)   : Rule   = label<Tag> + rule
label<Tag>(branch) : Branch = /* as above, except as branch */

id<Id>(rule)   : Rule   = id<Id> + rule
id<Id>(branch) : Branch = /* as above, except as branch */
----

For convenience, `label` and `id` have function call operators.
They produce the label/id and then parse the rule.

[discrete]
==== `lexy::dsl::capture`

.`lexy/dsl/capture.hpp`
----
capture(rule)   : Rule
capture(branch) : Branch
----

The `capture()` rule takes an arbitrary rule and parses it, capturing everything it has consumed into a `lexy::lexeme`.
It is a branch if given a branch.
It is whitespace aware; preceding whitespace is not captured.

[horizontal]
Branch Condition::
  The branch condition is whatever `branch` uses as a branch condition.
Matches::
  Matches and consumes whatever `rule` matches.
Values::
  A `lexy::lexeme` which begins at the original reader position and ends at the reader position after `rule` has been parsed,
  followed by any other values produced by parsing the `rule` in the same order.
Errors::
  All errors raised by `rule`. It cannot fail itself.

[godbolt,cpp,id=77jfM5]
----
// Captures the entire input.
dsl::capture(dsl::any)
----

[discrete]
==== `lexy::dsl::position`

.`lexy/dsl/position.hpp`
----
position : Rule
----

The `position` rule creates as its value an iterator to the current reader position without consuming any input.

[horizontal]
Matches::
  Any input, but does not consume anything.
Value::
  An iterator to the current position of the reader.
Error::
  n/a (it does not fail)

[godbolt,cpp,id=Wh86vn]
----
// Parses the entire input and returns the final position.
dsl::any + dsl::position
----

TIP: Use `position` when creating an AST whose nodes are annotated with their original source position.

=== Errors

The following rules are used to customize/improve error messages.

[discrete]
==== `.error<Tag>()`

----
token.error<Tag>() : Token
----

The `error()` function on tokens changes the error that is raised when a token failed.

[horizontal]
Matches::
  Matches and consumes what `token` matches.
Error::
  A generic error with the specified `Tag`.

TIP: It is useful for tokens such as `dsl::token()` and `operator-`, where the result is a generic tag such as `lexy::missing_token` or `lexy::minus_failure`.

[discrete]
==== `lexy::dsl::error`

.`lexy/dsl/error.hpp`
----
error<Tag>       : Branch
error<Tag>(rule) : Branch
----

The `error` rule always fails and produces an error with the given tag.
For the second version, the rule is matched first to determine the error range.

[horizontal]
Branch Condition::
  Branch is always taken.
Matches::
  Nothing and always fails.
Error::
  An error object of the specified `Tag`.
  If the optional `rule` is given, it will be matched (without producing values or errors).
  If it matched successfully, the previous and new reader position will be used to determine the error range.
  Otherwise, the error has no range.

TIP: Use it as the final branch of a choice rule to customize the `lexy::exhausted_choice` error.

[discrete]
==== `lexy::dsl::require` and `lexy::dsl::prevent`

.`lexy/dsl/error.hpp`
----
require<Tag>(rule) : Rule
prevent<Tag>(rule) : Rule
----

The `require` and `prevent` rules can be used to lookahead and fail if the input matches or does not match the token.

[horizontal]
Matches::
  Both match the `rule` without consuming input (or producing values or errors).
  `require` fails if the `rule` did not match; `rule` fails if it did.
Error::
  An error object of the specified `Tag`.

[godbolt,cpp,id=n7zM4d]
----
// Parses a sequence of digits but raises an error with tag `forbidden_leading_zero` if a zero is followed by more digits.
// Note: this is already available as `dsl::digits<>.no_leading_zero()`.
dsl::zero >> dsl::prevent<forbidden_leading_zero>(dsl::digits<>)
    | dsl::digits<>
----

TIP: Use `prevent` together with `times` to prevent the rule from matching more than the specified number of times.

=== Branch conditions

The following rules are designed to be used as the condition of an `operator>>`.
They have no effect if not used in a context that requires a branch.

[discrete]
==== `lexy::dsl::else_`

.`lexy/dsl/branch.hpp`
----
else_ : Branch
----

If `else_` is used as a condition, that branch will be taken unconditionally.
It must be used as a last alternative in a choice.

[discrete]
==== `lexy::dsl::peek`

.`lexy/dsl/peek.hpp`
----
peek(rule) : Branch
----

The `peek` branch is taken if `rule` matches, but does not consume it.
It is whitespace aware; leading whitespace will be consumed.

CAUTION: Long lookahead can slow down parsing speed due to backtracking.

[discrete]
==== `lexy::dsl::peek_not`

.`lexy/dsl/peek.hpp`
----
peek_not(rule) : Branch
----

The `peek_not()` branch is taken if `rule` does not match, but does not consume it.
It is whitespace aware; leading whitespace will be consumed.

CAUTION: Long lookahead can slow down parsing speed due to backtracking.

[discrete]
==== `lexy::dsl::lookahead`

.`lexy/dsl/lookahead.hpp`
----
lookahead(needle, end) : Branch
----

The `lookahead` branch is taken if lookahead finds `needle` before `end` is found, which must both be tokens.
No characters are consumed.
It is whitespace aware; leading whitespace will be consumed.

CAUTION: Long lookahead can slow down parsing speed due to backtracking.

=== Branches

[discrete]
==== `lexy::dsl::operator+`

.`lexy/dsl/sequence.hpp`
----
rule + rule   : Rule
token + token : Branch
----

A sequence rule matches multiple rules one after the other.

[horizontal]
Branch Condition::
  The branch is only taken if all the tokens match in sequence.
Matches::
  Matches and consume the first rule, then matches and consumes the second rule, and so on.
  Only succeeds if all of them succeed.
Values::
  All the values produced by the rules in the same order as they were matched.
Errors::
  Whatever errors are raised by the individual rules.

[discrete]
==== `lexy::dsl::operator>>`

.`lexy/dsl/branch.hpp`
----
branch >> rule : Branch
----

The `operator>>` is used to turn a rule into a branch by giving it a branch condition, which must be a branch itself.
If the branch is used as a normal rule, it first matches the condition followed by the rule.
If it is used in a context that requires a branch, the branch is checked to determine whether it should be taken.

[horizontal]
Branch Condition::
  Whatever `branch` uses as branch condition.
Matches::
  Matches and consume the branch, then matches and consumes the `rule`.
  Only succeeds if all of them succeed.
Values::
  All the values produced by the branch and rule in the same order as they were matched.
Errors::
  Whatever errors are raised by the individual branch and rule.

[discrete]
==== `lexy::dsl::if_`

.`lexy/dsl/if.hpp`
----
if_(branch) : Rule
----

The `if_` rule matches a branch only if its condition matches.

[horizontal]
Matches::
  First matches the branch condition.
  If that succeeds, consumes it and matches and consumes the rest of the branch.
  Otherwise, consumes nothing and succeeds anyway.
Values::
  Any values produced by the branch.
Errors::
  Any errors produced by the branch.
  It will only fail after the condition has been matched.

[godbolt,cpp,id=GaxjbP]
----
// Matches an optional C style comment.
dsl::if_(LEXY_LIT("/*") >> dsl::until(LEXY_LIT("*/")))
----

[discrete]
==== `lexy::dsl::opt`

.`lexy/dsl/opt.hpp`
----
opt(branch) : Rule = branch | else_ >> nullopt
----

The `opt` rule matches a branch only if its condition matches.
Unlike `if_`, if the branch was not taken, it produces a `lexy::nullopt`.

[horizontal]
Matches::
  First matches the branch condition.
  If that succeeds, consumes it and matches and consumes the rest of the branch.
  Otherwise, consumes nothing and succeeds anyway.
Values::
  If the branch condition matches, any values produced by the rule.
  Otherwise, a single object of type `lexy::nullopt`.
Errors::
  Any errors produced by the branch.
  It will only fail after the condition has been matched.

[godbolt,cpp,id=1vK39o]
----
// Matches an optional list of alpha characters.
// (The id<0> is just there, so the sink will be invoked on each character).
// If no items are present, it will default construct the list type.
dsl::opt(dsl::list(dsl::ascii::alpha >> dsl::id<0>))
----

[discrete]
==== `lexy::dsl::operator|`

.`lexy/dsl/choice.hpp`
----
branch  | branch  : Rule
----

A choice rule matches the first branch in order whose condition was matched.

[horizontal]
Matches::
  Tries to match the condition of each branch in the order they were specified.
  As soon as one branch condition matches, matches and consumes that branch without ever backtracking to try another branch.
  If no branch condition matched, fails without consuming anything.
Values::
  Any values produced by the selected branch.
Errors::
  Any errors raised by the then of the selected branch.
  If no branch condition matched, a generic error with tag `lexy::exhausted_choice`.

[godbolt,cpp,id=aaEnW7]
----
// A contrived example to illustrate the behavior of choice.
// Note that branch with id 1 will never be taken, as branch 0 takes everything starting with a and then fails if it isn't followed by bc.
// The correct behavior is illustrated with 2 and 3, there the branch with the longer condition is listed first.
dsl::id<0>(LEXY_LIT("a") >> LEXY_LIT("bc"))
  | dsl::id<1>(LEXY_LIT("a") >> LEXY_LIT("b"))
  | dsl::id<2>(LEXY_LIT("bc"))
  | dsl::id<3>(LEXY_LIT("b"))
----

NOTE: The C++ operator precedence is specified in such a way that `condition >> a | else_ >> b` works.
The compiler might warn that the precedence is not intuitive without parentheses, but in the context of this DSL it is the expected result.

TIP: Use `… | error<Tag>` to raise a custom error instead of `lexy::exhausted_choice`.

[discrete]
==== `lexy::dsl::operator/`

.`lexy/dsl/alternative.hpp`
----
token / token : Token
----

An alternative rule tries to match each token in order, backtracking if necessary.

[horizontal]
Matches::
  Tries to match each token in the order they were specified.
  As soon as one token matches, consumes it and succeeds.
  If no token matched, fails without consuming anything.
Errors::
  A generic error with tag `lexy::exhausted_alternatives` if no token matched.

NOTE: If an alternative consists of only literals, a trie is used to efficiently match them without backtracking.

CAUTION: Use a choice rule with a suitable condition to avoid potentially long backtracking.

[discrete]
==== `lexy::dsl::switch_`

.`lexy/dsl/switch.hpp`
----
switch_(rule) : Rule

switch_(rule).case_(branch)  : Rule
switch_(rule).default_(rule) : Rule = switch_(rule).case_(else_ >> rule)
switch_(rule).error<Tag>()   : Rule = switch_(rule).case_(error<Tag>(any))
----

The `switch_` rule matches a rule and then switches over the input the rule has consumed.
Switch cases can be added by calling `.case_()`; they are tried in order.
A default case is added using `.default_()`; it is taken unconditionally.
Alternatively, an error case can be added using `.error<Tag>()`; it produces an error if no previous case has matched.

[horizontal]
Matches::
  First matches and consumes the switched rule.
  What the rule has consumed is then taken as the entire input for matching the switch cases.
  Then it tries to match the branch conditions of each case in order.
  When a branch condition matches, that case is taken and its then is matched.
  If no case has matched, it fails.
Values::
  Any values produced by the switched rule followed by any values produced by the selected case.
Errors::
  If the switched rule fails to match, any errors raised by it.
  If the branch condition of a case has matched, any errors raised by the then.
  If the switch had an error case, a generic error with the specified `Tag` is raised whose range is everything consumed by the switched rule.
  Otherwise, a generic error with tag `lexy::exhausted_switch` is raised.

[godbolt,cpp,id=G87Mqf]
----
// Parse identifiers (one or more alpha numeric characters) but detect the three reserved keywords.
// We use `+ dsl::eof` in the case condition to ensure that `boolean` is not matched as `bool`.
dsl::switch_(dsl::while_one(dsl::ascii::alnum))
    .case_(LEXY_LIT("true")  + dsl::eof >> dsl::id<1>)
    .case_(LEXY_LIT("false") + dsl::eof >> dsl::id<2>)
    .case_(LEXY_LIT("bool")  + dsl::eof >> dsl::id<3>)
    .default_(dsl::id<0>) // It wasn't a reserved keyword but a normal identifier.

// Note: a more efficient and convenient method for handling keywords is planned.
----

NOTE: It does not matter if the then of a case does not consume everything the original rule has consumed.
As soon as the then has matched everything parsing continues from the reader position after the switched rule has been matched.

=== Loops

[discrete]
==== `lexy::dsl::until`

.`lexy/dsl/until.hpp`
----
until(token)          : Token
until(token).or_eof() : Token = until(token / eof)
----

The `until` token consumes all input until the specified `token` matches, then consumes that.

[horizontal]
Matches::
  If the closing `token` matches, consumes it and succeeds.
  Otherwise, consumes one code unit and tries again.
  If EOF is reached, fails, unless `.or_eof()` was called, in which case it also succeeds having consumed everything until the end of the input.
Errors::
  It can only fail if the reader has reached the end of the input without matching the condition.
  Then it raises the same error as raised if the condition would be matched at EOF.

[godbolt,cpp,id=Yn4WTj]
----
// Matches a C style comment.
// Note that we don't care what it contains.
LEXY_LIT("/*") >> dsl::until(LEXY_LIT("*/"))
----

NOTE: `until` includes the `token`.

[discrete]
==== `lexy::dsl::loop`

.`lexy/dsl/loop.hpp`
----
loop(rule) : Rule

break_ : Rule
----

The `loop` rule matches the given rule repeatedly until it either fails to match or a `break_` rule was matched.

[horizontal]
Requires::
  `rule` must not produce any values.
  `break_` must be used inside a loop.
Matches::
  While the rule matches, consumes it and repeats.
  If a `break_` is matched, parsing will stop immediately and it succeeds.
  If the rule does not match, it fails.
Values::
  No values are produced.
Errors::
  Any errors raised when the rule fails to match.

NOTE: The `loop` rule is mainly used to implement other rules.
It is unlikely that you are going to need it yourself.

WARNING: If `rule` contains a branch that will not consume any characters but does not break, `loop` will loop forever.

[discrete]
==== `lexy::dsl::while_`

.`lexy/dsl/while.hpp`
----
while_(branch) : Rule
----

The `while` rule matches a branch as long as it condition has matched.

[horizontal]
Requires::
  `branch` must not produce any values.
Matches::
  While the branch condition matches, matches and consumes the then then repeats.
  If the branch condition does not match anymore, succeeds without consuming additional input.
Values::
  No values are produced.
Errors::
  The rule can only fail if the then of the branch fails.
  Then it will raise its error unchanged.

WARNING: If the branch does not consume any characters, `while_` will loop forever.

'''

.`lexy/dsl/while.hpp`
----
while_(choice)         : Rule = loop(choice | else_ >> break_)
----

This version of the `while_` rule repeatedly matches a choice as long as one branch matches.

[horizontal]
Requires::
  `choice` most not produce any values.
Matches::
  While one branch condition of the choice matches, consumes it and the rest of the branch.
  If no branch matches anymore, succeeds without consuming additional input.
Values::
  No values are produced.
Errors::
  The rule can only fail if the then of one of the choice branches fails.
  Then it will raise its error unchanged.

WARNING: If the choice has an else branch already, it will loop until an error is raised.

[discrete]
==== `lexy::dsl::while_one()`

.`lexy/dsl/while.hpp`
----
while_one(rule)    : Rule    = rule + while_(rule)
while_one(branch)  : Branch  = branch >> while_(branch)
while_one(token)   : Token
----

The `while_one` rule matches a rule one or more times.
It is a token if it is given a token.

[discrete]
==== `lexy::dsl::do_while()`

.`lexy/dsl/while.hpp`
----
do_while(rule, condition_branch)   : Rule   = rule + while_(condition_branch >> rule)
do_while(branch, condition_branch) : Branch = branch >> while_(condition_branch >> rule)
----

The `do_while` rule matches a rule first unconditionally, and then again repeatedly while the rule matches.

[godbolt,cpp,id=4dzEK7]
----
// Equivalent to `dsl::list(dsl::ascii::alpha, dsl::sep(dsl::comma))` but does not produce a value.
dsl::do_while(dsl::ascii::alpha, dsl::comma)
----

[discrete]
==== `lexy::dsl::sep` and `lexy::dsl::trailing_sep`

.`lexy/dsl/separator.hpp`
----
sep(branch)
trailing_sep(branch)
----

`sep` and `trailing_sep` are used to specify a separator between repeated items; they are not rules that can be parsed directly.

Use `sep(branch)` to indicate that `branch` has to be consumed between two items.
If it would match after the last item, it is not consumed by the rule.

Use `trailing_sep(branch)` to indicate that `branch` has to be consumed between two items and can occur after the final item.
If it matches after the last item, it is consumed as well.

[discrete]
==== `lexy::dsl::times`

.`lexy/dsl/times.hpp`
[source,cpp]
----
namespace lexy
{
    template <std::size_t N, typename T>
    using times = T (&)[N];

    template <typename T>
    using twice = times<2, T>;
}
----

.`lexy/dsl/times.hpp`
----
times<N>(rule)      : Rule
times<N>(rule, sep) : Rule

twice(rule)      : Rule = times<2>(rule)
twice(rule, sep) : Rule = times<2>(rule, sep)
----

The `times` rule repeats the rule `N` times with optional separator in between and collects all produced values into an array.
The `twice` rule is a convenience alias for `N = 2`.

[horizontal]
Requires::
  The separator must not produce any values.
  All values produced by the parsing the rule must have a common type.
  In particular, the rule must only produce one value.
Matches::
  If no separator is specified, matches and consumes `rule` `N` times.
  If a separator is specified, matches and consumes `rule` `N` times, consuming the separator between two items and potentially after all items if the separator is trailing.
Values::
  Produces a single array containing `N` items which are all the values produced by each repetition.
  The typedef `lexy::times` or `lexy::twice` can be used to process that array.
Errors::
  All errors raised by matching the rule or separator.

[godbolt,cpp,id=hrTKaT]
----
// Parses an IPv4 address (4 uint8_t's seperated by periods).
dsl::times<4>(dsl::integer<std::uint8_t>(dsl::digits<>), dsl::sep(dsl::period))
----

[discrete]
==== `lexy::dsl::list`

.`lexy/dsl/list.hpp`
----
list(rule)   : Rule
list(branch) : Branch

list(rule, sep)   : Rule
list(branch, sep) : Branch
----

The `list` rule matches a rule one or more times, optionally separated by a separator.
Values produced by the list items are forwarded to a sink callback.

[horizontal]
Branch Condition::
  Whatever `branch` uses as branch condition.
Requires::
  The item rule must be a branch or a choice rule unless a non-trailing separator is used (in that case the separator can be used as condition).
  A production whose rule contains `list()` must provide a sink.
Matches::
  Matches and consumes the item rule one or more times.
  In between items and potentially after the final item, a separator is matched and consumed if provided according to its rules.
  If the separator is provided and non-trailing, the existence of a separator determines whether or not the rule should be matched again.
  Otherwise, the branch condition of the branch rule or an added else branch of the choice rule is used to determine that.
Values::
  Only a single value, which is the result of the finished sink.
  Every time the item rule is parsed, all values it produces are passed to the sink which is invoked once per iteration.
  If the separator is captured, its lexeme is also passed to the sink, but in a separate invocation.
Errors::
  All errors raised when parsing the item rule or separator.

[godbolt,cpp,id=sE873v]
----
// Parses a list of integers seperated by (a potentially trailing) comma.
// As the separator is trailing, it cannot be used to determine the end of the list.
// As such we peek whether the input contains a digit in our item condition.
// The sink is invoked with each integer.
dsl::list(dsl::peek(dsl::digit<>) >> dsl::integer<int>(dsl::digits<>),
          dsl::trailing_sep(dsl::comma))
----

TIP: Use one of the bracketing rules if your list item does not have an easy condition and the list is surrounded by given tokens anyway.

[discrete]
==== `lexy::dsl::combination`

.`lexy/dsl/combination.hpp`
----
combination(branch1, branch2, ...) : Rule
combination<Tag>(branch1, branch2, ...) : Rule
----

The `combination` rule matches each of the sub-rules exactly once but in any order.
Values produced by the rules are forwarded to a sink.

[horizontal]
Requires::
  A production whose rule contains `combination()` must provide a sink.
Matches::
  Matches and consumes all rules in an arbitrary order.
  This is done by parsing the choice created from the branches exactly `N` times.
  Branches that have already been taken are not excluded on future iterations.
  If they are taken again, the rule fails.
Values::
  Only a single value, which is the result of the finished sink.
  All values produced by the branches are passed to the sink which is invoked once per iteration.
Errors::
  All errors raised by parsing the branches.
  If no branch is matched, but there are still missing branches,
  a generic error with tag `lexy::exhausted_choice` is raised.
  If a branch is matched twice, a generic error is raised.
  It has the specified tag or `lexy::combination_duplicate` if none was specified.

[godbolt,cpp,id=bjKqvj]
----
// Matches 'a', 'b', or 'c', in any order.
dsl::combination(dsl::lit_c<'a'>, dsl::lit_c<'b'>, dsl::lit_c<'c'>)
----

WARNING: The branches are tried in order. If an earlier branch always takes precedence over a later one, the combination can never be successful.

[discrete]
==== `lexy::dsl::partial_combination`

.`lexy/dsl/combination.hpp`
----
partial_combination(branch1, branch2, ...) : Rule
partial_combination<Tag>(branch1, branch2, ...) : Rule
----

The `partial_combination` rule matches each of the sub-rules at most once but in any order.
Values produced by the rules are forwarded to a sink.

[horizontal]
Requires::
  A production whose rule contains `partial_combination()` must provide a sink.
Matches::
  Matches and consumes a subset of the rules in an arbitrary order.
  This is done by parsing the choice created from the branches exactly `N` times.
  Branches that have already been taken are not excluded on future iterations.
  If they are taken again, the rule fails.
  If no branch is taken, the rule succeeds.
Values::
  Only a single value, which is the result of the finished sink.
  All values produced by the branches are passed to the sink which is invoked once per iteration.
Errors::
  All errors raised by parsing the branches.
  If a rule is matched twice, a generic error is raised.
  It has the specified tag or `lexy::combination_duplicate` if none was specified.

[godbolt,cpp,id=85dv9W]
----
// Matches a subset of 'a', 'b', or 'c', in any order.
dsl::partial_combination(dsl::lit_c<'a'>, dsl::lit_c<'b'>, dsl::lit_c<'c'>)
----

WARNING: The branches are tried in order. If an earlier branch always takes precedence over a later one, the combination can never be successful.

=== Productions

Every rule is owned by a production.
The following rules allow interaction with other productions.

[discrete]
==== `lexy::dsl::p` and `lexy::dsl::recurse`

.`lexy/dsl/production.hpp`
----
p<Production> : Rule or Branch
recurse<Production> : Rule
----

The `p` and `recurse` rules parses the rule of another production.
The `p` rule is a branch, if the rule of the other production is a branch.
Both are whitespace aware.

[horizontal]
Requires::
  For `p`, the `Production` is a complete type at the point of the rule definition.
  The `recurse` rule has no such limitations.
Branch Condition::
  Whatever the production's rule uses as a branch condition.
Matches::
  Matches and consumes `Production::rule`.
Values::
  A single value, which is the result of parsing the production.
  All values produced by parsing its rule are forwarded to the productions value callback.
Errors::
  If matching fails, `Production::rule` will raise an error which is handled in the context of `Production`.
  This results in a failed result object, which is converted to our result type and returned.

[godbolt,cpp,id=oj9T3n]
----
// Parse a sub production followed by an exclamation mark.
dsl::p<sub_production> + dsl::lit_c<'!'>
----

TIP: While `recurse` can be used to implement direct recursion (e.g. `prefix >> dsl::p<current_production> | dsl::else_ >> end` to match zero or more `prefix` followed by `end`), it is better to use loops instead.

WARNING: Left recursion will create an infinite loop.

[discrete]
==== `lexy::dsl::return_`

.`lexy/dsl/return.hpp`
----
return_ : Rule
----

Conceptually, each production has an associated function that parses the specified rule.
The `return_` rule will exit that function early, without parsing subsequent rules.

[horizontal]
Requires::
  It must not be used inside loops.
Matches::
  Any input, but does not consume anything.
  Subsequent rules are not matched further.
Values::
  It does not produce any values, but all values produced so far are forwarded to the callback.
Errors::
  n/a (it does not fail)

[godbolt,cpp,id=zrbcaq]
----
// Match an opening parenthesis followed by 'a' or 'b'.
// If it is followed by 'b', the closing parenthesis is not matched anymore.
dsl::parenthesized(dsl::lit_c<'a'> | dsl::lit_c<'b'> >> dsl::return_)
----

CAUTION: When using `return_` together with the context sensitive parsing facilities, remember to pop all context objects before the return.

=== Brackets and terminator

[discrete]
==== Terminator

.`lexy/dsl/terminator.hpp`
----
terminator(branch)
terminator(branch).terminator() : Branch = branch
----

A terminator can be specified using `terminator()`.
The result is not a rule, but a DSL for specifying that a rule is followed by the terminator.
The terminator is defined using a branch; it is returned by calling `.terminator()`.

.`lexy/dsl/terminator.hpp`
----
t[rule]
----

Whitespace can be added by writing `t[rule]`, where `t` is the result of a `terminator()` call.
It will skip whitespace before matching the terminator.
If whitespace is added, this is also reflected by `.terminator()`.

.`lexy/dsl/terminator.hpp`
----
t(rule) : Rule = rule + t.terminator()
----

Calling `t(rule)`, where `t` is the result of a `terminator()` call, results in a rule that parses the given `rule` followed by the terminator.

.`lexy/dsl/terminator.hpp`
----
t.while_(rule) : Rule
t.while_one(rule) : Rule

t.opt(rule) : Rule

t.list(rule) : Rule
t.list(rule, sep) : Rule

t.opt_list(rule) : Rule
t.opt_list(rule, sep) : Rule
----

Using `t.while_()`, `t.while_one()` `t.opt()`, `t.list()`, or `t.opt_list()`, where `t` is the result of a `terminator()` call, results in a rule that parses `while_(rule)`, `while_one(rule)`, `opt(rule)`, `list(rule)` and `opt(list(rule))`, respectively, but followed by the terminator.
The `rule` does not need to be a terminator is used as the branch condition for the `while_()`, `opt()` and `list()` rule.

[discrete]
==== Brackets

.`lexy/dsl/brackets.hpp`
----
brackets(open_branch, close_branch)
brackets(open_branch, close_branch).open()  : Branch = open_branch
brackets(open_branch, close_branch).close() : Branch = close_branch
----

A set of open and close brackets can be specified using `brackets()`.
The result is not a rule, but a DSL for specifying that a rule is surrounded by brackets.
The open and close brackets are defined using branches; they are returned by calling `.open()` and `.close()`.

.`lexy/dsl/brackets.hpp`
----
b[rule]
----

Whitespace can be added by writing `b[rule]`, where `b` is the result of a `brackets()` call.
It will skip whitespace before matching the open and close branch.
If whitespace is added, this is also reflected by `.open()` and `.close()`.

.`lexy/dsl/brackets.hpp`
----
b(rule) : Branch = b.open() >> rule + b.close()
----

Calling `b(rule)`, where `b` is the result of a `brackets()` call, results in a rule that parses the given `rule` surrounded by brackets.
The rule is a branch that uses the opening bracket as a branch condition.

.`lexy/dsl/brackets.hpp`
----
b.while_(rule) : Branch
b.while_one(rule) : Branch

b.opt(rule) : Branch

b.list(rule) : Branch
b.list(rule, sep) : Branch

b.opt_list(rule) : Branch
b.opt_list(rule, sep) : Branch
----

Using `b.while_()`, `b.while_one()` `b.opt()`, `b.list()`, or `b.opt_list()`, where `b` is the result of a `brackets()` call, results in a branch that parses `while_(rule)`, `while_one(rule)`, `opt(rule)`, `list(rule)` and `opt(list(rule))`, respectively, but surrounded as brackets.
The `rule` does not need to be a branch as the closing brackets is used as the branch condition for the `while_()`, `opt()` and `list()` rule.

.`lexy/dsl/brackets.hpp`
----
round_bracketed  = brackets(lit_c<'('>, lit_c<')'>)
square_bracketed = brackets(lit_c<'['>, lit_c<']'>)
curly_bracketed  = brackets(lit_c<'{'>, lit_c<'}'>)
angle_bracketed  = brackets(lit_c<'<'>, lit_c<'>'>)

parenthesized = round_bracketed
----

Common sets of open and close brackets are pre-defined.

[godbolt,cpp,id=G9MPKh]
----
// Parses a list of integers seperated by (a potentially trailing) comma surrounded by parentheses.
// The same example without the parentheses was also used for list,
// but we required a list condition that needed to perform lookahead.
// Now, the closing parentheses is used as the condition and we don't need to lookahead.
dsl::parenthesized.list(dsl::integer<int>(dsl::digits<>),
                        dsl::trailing_sep(dsl::comma))
----

=== Numbers

The facilities for parsing integers are split into the digit token, which do not produce any values,
and the `integer` rule, which matches a digit token and converts it into an integer.
The integer conversion has to be done during and parsing and not as a callback, as overflow creates a parse error.

[discrete]
==== Base

.`lexy/dsl/digit.hpp`
[source,cpp]
----
namespace lexy::dsl
{
    struct binary;
    struct octal;
    struct decimal;
    struct hex_lower;
    struct hex_upper;
    struct hex;
}
----

The set of allowed digits and their values is specified using a `Base`, which is a policy class passed to the rules.

`binary`::
  Matches the base 2 digits `0` and `1`.
`octal`::
  Matches the base 8 digits `0-7`.
`decimal`::
  Matches the base 10 digits `0-9`. If no base is specified, this is the default.
`hex_lower`::
  Matches the lower-case base 16 digits `0-9` and `a-f`.
`hex_upper`::
  Matches the upper-case base 16 digits `0-9` and `A-F`.
`hex`::
  Matches the base 16 digits `0-9`, `A-F`, and `a-f`.

[discrete]
==== `lexy::integer_traits`

.`lexy/dsl/integer.hpp`
----
namespace lexy
{
    template <typename T>
    struct integer_traits
    {
        using type = T;

        static constexpr bool is_bounded;

        template <int Radix>
        static constexpr std::size_t max_digit_count;

        template <int Radix>
        static constexpr void add_digit_unchecked(type& result, unsigned digit);
        template <int Radix>
        static constexpr bool add_digit_checked(type& result, unsigned digit)
    };

    template <>
    struct integer_traits<lexy::code_point>;

    template <typename T>
    struct unbounded
    {};
    template <typename T>
    struct integer_traits<unbounded<T>>
    {
        using type                       = typename integer_traits<T>::type;
        static constexpr bool is_bounded = false;

        template <int Radix>
        static constexpr void add_digit_unchecked(type& result, unsigned digit);
    };
}
----

The `lexy::integer_traits` are used for parsing an integer.
It controls its maximal value and abstracts away the required integer operations.

The `type` member is the actual type that will be returned by the parse operation. It is usually `T`.
The parsing algorithm does not require that `type` is an integer type, it only needs to have a constructor that initializes it from an `int`.
If `is_bounded` is `true`, parsing requires overflow checking.
Otherwise, parsing does not require overflow checking and `max_digit_count` and `add_digit_checked` are not required.
`max_digit_count` returns the number of digits necessary to express the bounded integers maximal value in the given radix.
It must be bigger than `1`.
`add_digit_unchecked` and `add_digit_checked` add `digit` to result by doing the equivalent of `result = result * Radix + digit`.
The `_checked` version returns `true` if that has lead to an integer overflow.

The primary template works with any integer type and there is a specialization for `lexy::code_point`.
By wrapping your integer type in `lexy::unbounded`, you can disable bounds checking during parsing.
It specialization of `lexy::integer_traits` is built on top of the specialization of `lexy::integer_traits<T>`,
but disables all bounds checking.
You can specialize `lexy::integer_traits` for your own integer types.

[discrete]
==== `lexy::dsl::zero`

.`lexy/dsl/digit.hpp`
----
zero : Token
----

The `zero` token matches the zero digit.

[horizontal]
Matches::
    Matches and consumes the zero digit `0`.
Errors::
    Raises a `lexy::expected_char_class` error with the name `digit.zero`.

[discrete]
==== `lexy::dsl::digit`

.`lexy/dsl/digit.hpp`
----
digit<Base> : Token
----

The `digit` token matches a digit of the specified base or `decimal` if no base was specified.

[horizontal]
Matches::
    Matches and consumes any of the valid digits of the base.
Errors::
    Raises a `lexy::expected_char_class` error with the name `digit.<base>`, where `<base>` is `binary`, `hex-lower`, etc.

[discrete]
==== `lexy::dsl::digits`

.`lexy/dsl/digit.hpp`
----
digits<Base> : Token

digits<Base>.sep(token)        : Token
digits<Base>.no_leading_zero() : Token
----

The `digits` token matches a non-empty sequence of digits in the specified base or `decimal` if no base was specified.
Calling `.sep()` allows adding a digit separator token that can be present at any point between two digits, but is not required.
Calling `.no_leading_zero()` raises an error if one or more leading zeros are encountered.
The calls to `.sep()` and `.no_leading_zero()` can be chained.

[horizontal]
Matches::
  Matches and consumes one or more digits of the specified base.
  If a separator was added, it tries to match it after every digit.
  It is consumed if it was matched, but it does not fail if no separator was present.
  If a separator is matched without a following digit, it fails.
  If `.no_leading_zero()` was called, fails if the first digit was zero and it is followed by another digit or separator.
  If it could not match any more digits after the initial one, matching succeeds.
Errors::
  All errors raised by `digit<Base>`, which can only happen for the initial digit.
  Raises a generic error with tag `lexy::forbidden_leading_zero` if a leading zero was matched.

[godbolt,cpp,id=Kq1vez]
----
// Matches upper-case hexadecimal digits seperated by ' without leading zeroes.
dsl::digits<dsl::hex_upper>.sep(dsl::digit_sep_tick).no_leading_zero()
----

NOTE: The separator can be placed at any point between two digits.
There is no validation of rules to ensure it is a thousand separator or similar conventions.

'''

.`lexy/dsl/digit.hpp`
----
digit_sep_underscore : Token = lit<"_">
digit_sep_tick       : Token = lit<"'">
----

For convenience, two common digit separators `_` and `'` are predefined as `digit_sep_underscore` and `digit_sep_tick` respectively.
However, the digit separator can be an arbitrarily complex token.

[discrete]
==== `lexy::dsl::n_digits`

.`lexy/dsl/digit.hpp`
----
n_digits<N, Base> : Token

n_digits<N, Base>.sep(token) : Token
----

The `n_digits` token matches exactly `N` digits in the specified base or `decimal` if no base was specified.
Calling `.sep()` allows adding a digit separator token that can be present at any point between two digits, but is not required.

[horizontal]
Matches::
  Matches and consumes exactly `N` digits of the specified base.
  If a separator was added, it tries to match it after every digit.
  It is consumed if it was matched, but it does not fail if no separator was present.
  If a separator is matched without a following digit, it fails.
  Separators do not count towards the digit count.
Errors::
  All errors raised by `digit<Base>`, which can happen if less than `N` digits are available.
  Raises a generic error with tag `lexy::forbidden_leading_zero` if a leading zero was matched.

[godbolt,cpp,id=1YcrGa]
----
// Matches 4 upper-case hexadecimal digits seperated by '.
dsl::n_digits<4, dsl::hex_upper>.sep(dsl::digit_sep_tick)
----

[discrete]
==== `lexy::dsl::integer`

.`lexy/dsl/integer.hpp`
----
integer<T, Base>(token) : Rule
----

The `integer` rule converts the lexeme matched by the `token` into an integer of type `T` using the given base.
The `Base` can be omitted if the token is `digits` or `n_digits`.
It will then be deduced from the token.

[horizontal]
Matches::
  Matches and consumes what `token` matches.
Values::
  An integer of type `T` that is created by the characters the token has consumed.
  If the token matches characters that are not valid digits of the base (e.g. a digit separator), those characters are skipped.
  Otherwise, the character is converted to a digit and added to the resulting integer using the `lexy::integer_traits`.
Errors::
  Any errors raised by matching the token.
  If the integer type `T` is bounded and the integer value would overflow, a generic error with tag `lexy::integer_overflow` is raised.

[godbolt,cpp,id=6ThWPn]
----
// Matches upper-case hexadecimal digits seperated by ' without leading zeroes.
// Converts them into an integer, the base is deduced from the token.
dsl::integer<int>(dsl::digits<dsl::hex_upper>
                        .sep(dsl::digit_sep_tick).no_leading_zero())
----

[discrete]
==== `lexy::dsl::code_point_id`

.`lexy/dsl/integer.hpp`
----
code_point_id<N, Base> : Rule = integer<lexy::code_point>(n_digits<N, Base>)
----

The `code_point_id` rule is a convenience rule that parses a code point.
It matches `N` digits in the specified base, which defaults to `hex`, and converts it into a code point.

[horizontal]
Matches::
  Matches and consumes exactly `N` digits of the specified base.
Values::
  The `lexy::code_point` that is specified using those digits.
Errors::
  The same error as `digit<Base>` if fewer than `N` digits are available.
  A generic error with tag `lexy::integer_overflow` if the code point value would exceed the maximum code point.

[discrete]
==== `lexy::dsl::plus_sign`, `lexy::dsl::minus_sign`, and `lexy::dsl::sign`

.`lexy/dsl/sign.hpp`
----
plus_sign  : Rule
minus_sign : Rule

sign : Rule
----

The `plus_sign`, `minus_sign`, and `sign` rule match an optional sign.

[horizontal]
Matches::
  `plus_sign`:::
    Matches and consumes a `+` character, if there is one.
  `minus_sign`:::
    Matches and consumes a `-` character, if there is one.
  `sign`:::
    Matches and consumes a `+` or `-` character, if there is one.
Values:::
  If a `+` sign was consumed, the value is `+1`.
  If a `-` sign was consumed, the value is `-1`.
  If no sign was consumed, the value is `+1`.
Errors::
  n/a (they don't fail)

[godbolt,cpp,id=7exP55]
----
// Parse a decimal integer with optional minus sign.
dsl::minus_sign + dsl::integer<int>(dsl::digits<>)
----

TIP: The callback `lexy::as_integer` takes the value produced by the sign rules together with an integer produced by the `integer` rule and negates it if necessary.

=== Delimited and quoted

.`lexy/dsl/delimited.hpp`
----
delimited(open_branch, close_branch)
delimited(open_branch, close_branch).open()  : Branch = open_branch
delimited(open_branch, close_branch).close() : Branch = close_branch
----

A set of open and close delimiters can be specified using `delimited()`.
The result is not a rule, but a DSL for specifying a sequence of code points to be matched between the delimiters.
The open and close delimiters are defined using branches; they are returned by calling `.open()` and `.close()`.

.`lexy/dsl/delimited.hpp`
----
delimited(branch) = delimited(branch, branch)
----

There is a convenience overload if the same rule is used for the open and closing delimiters.

.`lexy/dsl/delimited.hpp`
----
d[rule]
----

Whitespace can be added by writing `d[rule]`, where `d` is the result of a `delimiter()` call.
It will skip whitespace before the opening delimiter only.
If whitespace is added, this is also reflected by `.open()`.

.`lexy/dsl/delimited.hpp`
----
quoted        = delimited(lit<"\"">)
triple_quoted = delimited(lit<"\"\"\"">)

single_quoted = delimited(lit<"'">)

backticked        = delimited(lit<"`">)
double_backticked = delimited(lit<"``">)
triple_backticked = delimited(lit<"```">)
----

Common delimiters are predefined.

NOTE: The naming of `quoted`, `triple_quoted` and `single_quoted` is not very logical, but reflects common usage.

[discrete]
==== Simple delimited

.`lexy/dsl/delimited.hpp`
----
d(rule)  : Branch
d(token) : Branch = d(capture(token))
----

Calling `d(rule)`, where `d` is the result of a `delimiter()` call, results in a rule that matches `rule` as often as possible surrounded by the delimiters.
Values produced by the `rule` are forwarded to a sink callback.

For convenience, if passing a token, the token is captured.
Otherwise, nothing would be passed to the sink.

[horizontal]
Requires::
  A production whose rule contains a delimited rule must provide a sink.
Branch Condition::
  Whatever the opening delimiter uses as branch condition.
Matching::
  Matches and consumes the opening delimiter, followed by zero or more occurrences of `rule`, followed by the closing delimiter.
  It determines whether or not to parse another instance of `rule` using the condition of the closing delimiter.
Values::
  Values produced by the opening delimiter, the finished sink (which might be empty), and values produced by the closing delimiter.
  Every time the rule is parsed, all values produced by it are passed to the sink.
Errors::
  All errors raised when matching the opening delimiter and the rule.
  If EOF is reached without a closing delimiter, a generic error with tag `lexy::missing_delimiter` is raised.

[godbolt,cpp,id=nnoMYv]
----
// Match a string consisting of code points that aren't control characters.
dsl::quoted(dsl::code_point - dsl::ascii::control)
----

[discrete]
==== Delimited with escape sequences

.`lexy/dsl/delimited.hpp`
----
d(rule, escape_branch)  : Branch
  = d(escape_branch | else_ >> rule)
d(token, escape_branch) : Branch
  = d(escape_branch | else_ >> capture(token))

d(rule, escape_choice)  : Branch
  = d(escape_choice | else_ >> rule)
d(token, escape_choice) : Branch
  = d(escape_choice | else_ >> capture(token))
----

There is a convenience overload to specify escape sequences in the delimited.
The `choice` matches all appropriate escape sequences and produces their values.

Calling `d(rule, escape)`, where `d` is the result of a `delimiter()` call, is equivalent to `d(escape | else_ >> rule)`, so it results in a rule that matches `escape | else_ >> rule` as often as possible surrounded by the delimiters.
Values produced by the `rule` or `escape` are forwarded to a sink callback.

For convenience, if passing a token, the token is captured.
Otherwise, nothing would be passed to the sink.

[godbolt,cpp,id=vqsfM4]
----
// Match a string consisting of code points that aren't control characters.
// `\"` can be used to add a `"` to the string.
dsl::quoted(dsl::code_point - dsl::ascii::control,
            LEXY_LIT("\\\"") >> dsl::value_c<'"'>)
----

NOTE: The closing delimiter is used as termination condition here as well.
If the escape sequence starts with a closing delimiter, it will not be matched.

[discrete]
==== `lexy::dsl::escape()`

.`lexy/dsl/delimited.hpp`
----
escape(token) : Rule
----

For convenience, the `escape` rule can be used to specify the escape token.

An escape rule consists of a leading token that matches the escape character (e.g. `\`), and zero or more alternatives for characters that can be escaped.
It then is equivalent to `token >> (alt0 | alt1 | alt2 | error<lexy::invalid_escape_sequence>)`.
It will only be considered after the leading token has been matched and then tries to match one of the alternatives.
If no alternative matches, it raises a generic error with tag `lexy::invalid_escape_sequence`.

.`lexy/dsl/delimited.hpp`
----
e.rule(branch) : Rule
  = escape_token >> ( ... | branch
                      | else_ >> error<lexy::invalid_escape_sequence>)
----

Calling `e.rule(branch)`, where `e` is an escape rule, adds `branch` to the end of the choice.

.`lexy/dsl/delimited.hpp`
----
e.capture(token) : Rule
  = escape_token >> (... | capture(token)
                      | else_ >> error<lexy::invalid_escape_sequence>)
----

Calling `e.capture(token)`, where `e` is an escape rule, adds an escape sequence that matches and captures token to the end of the choice.

.`lexy/dsl/delimited.hpp`
----
e.lit<Str>(rule) : Rule
  = escape_token >> (... | lit<Str> >> rule
                      | else_ >> error<lexy::invalid_escape_sequence>)
e.lit<Str>() : Rule
  = e.lit<Str>(value_str<Str>)

e.lit_c<C>(rule) : Rule
  = escape_token >> (... | lit_c<C> >> rule
                      | else_ >> error<lexy::invalid_escape_sequence>)
e.lit_c<C>() : Rule
  = e.lit_c<C>(value_c<C>)
----

Calling `e.lit()` or `e.lit_c()`, where `e` is an escape rule, adds an escape sequences that matches the literal and produces the values of the rule to the end of the choice.
If no rule is specified, it defaults to producing the literal itself.

.`lexy/dsl/delimited.hpp`
----
backslash_escape = escape(lit_c<'\\'>)
dollar_escape    = escape(lit_c<'$'>)
----

Common escape characters are predefined.

[godbolt,cpp,id=71EEWY]
----
// Match a string consisting of code points that aren't control characters.
// `\"` can be used to add a `"` to the string.
// `\uXXXX` can be used to add the code point with the specified value.
dsl::quoted(dsl::code_point - dsl::ascii::control,
            dsl::backslash_escape
              .lit_c<'"'>()
              .rule(dsl::lit_c<'u'> >> dsl::code_point_id<4>)
----

=== Aggregates

.`lexy/dsl/member.hpp`
----
member<MemPtr> = rule   : Rule
member<MemPtr> = branch : Branch

LEXY_MEM(Name) = rule   : Rule
LEXY_MEM(Name) = branch : Branch
----

The `member` rule together with the `lexy::as_aggregate<T>` callback assigns the values produced by the rule given to it via `=` to the specified member of the aggregate `T`.

[horizontal]
Requires::
  A production that contains a member rule needs to use `lexy::as_aggregate<T>` as sink or callback.
  The rule must produce exactly one value.
Matches::
  Matches and consumes the `rule` given to it via `=`.
Values::
  Produces two values.
  The first value identifiers the targeted member.
  For `member<MemPtr>`, this is the member pointed to by the member pointer.
  For `LEXY_MEM(Name)`, it is the member with the given `Name`.
  The second value is the value produced by the rule.
Errors::
  All errors raised during parsing of the assigned rule.

The `lexy::as_aggregate<T>` callback, collects all member and value pairs.
It then constructs an object of type `T` using value initialization and for each pair assigns the value to the specified member of it.
This works either as callback or a sink.
If a member is specified more than once, the final value is stored at the end.

[godbolt,cpp,id=EMYGx1]
----
// Parses two integers separated by commas.
// The first integer is assigned to a member called `second`,
// the second integer is assigned to a member called `first`.
(LEXY_MEM(second) = dsl::integer<int>(dsl::digits<>))
+ dsl::comma
+ (LEXY_MEM(first) = dsl::integer<int>(dsl::digits<>))
----

=== Context sensitive parsing

To parse context sensitive grammars, `lexy` allows the creation of _context variables_.
They allow to save state between different rules which can be used to parse context sensitive elements such as XML with matching opening and closing tag names.

A context variable has a type, which is limited to `bool`, `int` and `lexy::lexeme`, and an identifier, which is given by a type.
Before a variable can be used it needs to be created with `.create()`.
It is then available for all rules of the current production: child and parent production cannot access them.
Variables are not persistent between multiple invocations of a production;
every time a production is parsed it starts out with no variables.

See `example/xml.cpp` for an example that uses the context sensitive parsing facilities.

[discrete]
==== `lexy::dsl::context_flag`

.`lexy/dsl/context_flag.hpp`
----
context_flag<Id>
----

A `lexy::dsl::context_flag` controls a boolean that can be `true` or `false`.
Each object is uniquely identified by the type `Id`.
It is not a rule but a DSL for specifying operations which are then rules.

----
context_flag<Id>.create() : Rule
context_flag<Id>.create<Value>() : Rule
----

The `.create()` rule does not interact with the input at all.
When it is parsed, it creates the flag with the given `Id` and initializes it to the `Value` (defaulting to `false`).

----
context_flag<Id>.set()   : Rule
context_flag<Id>.reset() : Rule
----

The `.set()`/`.reset()` rules do not interact with the input at all.
When they are parsed, they set the flag with the given `Id` to `true`/`false` respectively.

----
context_flag<Id>.toggle() : Rule
----

The `.toggle()` rule does not interact with the input at all.
When it is parsed, it toggles the value of the flag with the given `Id`.

----
context_counter<Id>.select(rule_true, rule_false) : Rule
----

The `.select()` rule selects on the given rules depending on the value of the flag with the given `Id`.
It then parses the selected rule.

[horizontal]
Matches::
  If the value of the flag is `true`, matches and  consumes `rule_true`.
  Otherwise, matches and  consumes `rule_false`.
Values::
  All values produced by parsing the selected rule.
Errors::
  All errors raised by parsing the selected rule.

----
context_flag<Id>.require<ErrorTag>()        : Rule
context_flag<Id>.require<Value, ErrorTag>() : Rule
----

The `.require()` rule does not interact with the input at all.
When it is parsed, it checks that the value of the flag with the given `Id` is the given `Value` (defaults to `true`).
If that is the case, parsing continues.
Otherwise, the rule fails, producing an error with the given `ErrorTag`.

[discrete]
==== `lexy::dsl::context_counter`

.`lexy/dsl/context_counter.hpp`
----
context_counter<Id>
----

A `lexy::dsl::context_counter` controls a C++ `int`.
Each object is uniquely identified by the type `Id`.
It is not a rule but a DSL for specifying operations which are then rules.

----
context_counter<Id>.create() : Rule
context_counter<Id>.create<Value>() : Rule
----

The `.create()` rule does not interact with the input at all.
When it is parsed, it creates the counter with the given `Id` and initializes it to the `Value` (defaulting to `0`).

----
context_counter<Id>.inc() : Rule
context_counter<Id>.dec() : Rule
----

The `.inc()`/`.dec()` rules do not interact with the input at all.
When they are parsed, they increment/decrement the counter with the given `Id` respectively.

----
context_counter<Id>.push(rule) : Rule
context_counter<Id>.pop(rule)  : Rule
----

The `.push()`/`.pop()` rules parse the given `rule`.
The counter with the given `Id` is then incremented/decremented by the number of characters (code units) consumed by `rule`.

[horizontal]
Matches::
  Matches and  consumes `rule`.
Values::
  All values produced by parsing `rule`.
Errors::
  All errors raised by parsing `rule`.

----
context_counter<Id>.compare<Value>(rule_less, rule_eq, rule_greater) : Rule
----

The `.compare()` rule compares the value of the counter with the given `Id` to `Value`.
It then parses one of the three given rules, depending on the result.

[horizontal]
Matches::
  If the value of the counter is less than `Value`, matches and  consumes `rule_less`.
  If the value of the counter is equal than `Value`, matches and  consumes `rule_eq`.
  If the value of the counter is greater than `Value`, matches and  consumes `rule_greater`.
Values::
  All values produced by parsing the selected rule.
Errors::
  All errors raised by parsing the selected rule.

----
context_counter<Id>.require<ErrorTag>()        : Rule
context_counter<Id>.require<Value, ErrorTag>() : Rule
----

The `.require()` rule does not interact with the input at all.
When it is parsed, it checks that the value of the counter with the given `Id` is the given `Value` (defaults to `0`).
If that is the case, parsing continues.
Otherwise, the rule fails, producing an error with the given `ErrorTag`.

[discrete]
==== `lexy::dsl::context_lexeme`

.`lexy/dsl/context_lexeme.hpp`
----
context_lexeme<Id>
----

A `lexy::dsl::context_flag` controls a `lexy::lexeme` (i.e. a string view on part of the input).
Each object is uniquely identified by the type `Id`.
It is not a rule but a DSL for specifying operations which are then rules.

----
context_lexeme<Id>.create() : Rule
----

The `.create()` rule does not interact with the input at all.
When it is parsed, it creates the lexeme with the given `Id` and initializes it to an empty view.

----
context_lexeme<Id>.capture(rule) : Rule
----

The `.capture()` rule parses the given `rule`.
The lexeme with the given `Id` is then set to view everything the `rule` has consumed as-if `lexy::dsl::capture()` was used.

[horizontal]
Matches::
  Matches and  consumes `rule.`
Values::
  All values produced by parsing `rule`.
Errors::
  All errors raised by parsing `rule`.

----
context_lexeme<Id>.require<ErrorTag>(rule) : Rule
----

The `.require()` rule parses the given `rule`, capturing it in a temporary lexeme.
The temporary lexeme is then compared with the lexeme given by the `Id`.
If the two lexemes are equal, parsing continues.
Otherwise, the rule fails, producing an error with the given `ErrorTag`.

[horizontal]
Matches::
  Matches and  consumes `rule.`
Values::
  Discards values produced by `rule`.
Errors::
  All errors raised by parsing `rule`.

=== Raw input

The following facilities are meant for parsing input that uses the `lexy::raw_encoding`, that is input consisting of bytes, not text.

[discrete]
==== `lexy::dsl::bom`

.`lexy/dsl/bom.hpp`
----
bom<Encoding, Endianness> : Token
----

The `bom` token matches the byte-order mark (BOM) for the given encoding and `lexy::encoding_endianness`.

[horizontal]
Requires::
  `Endianness` is `lexy::encoding_endianness::little` or `lexy::encoding_endianness::big`.
Matches::
  If the encoding has a BOM, matches and consumes the BOM written in the given endianness.
Errors::
  A `lexy::expected_char_class` error with the name `BOM.<encoding>-<endianness>` if the BOM was not matched.

[godbolt,cpp,id=xbnEYs]
----
// Matches the UTF-16 big endian BOM (0xFE, 0xFF).
dsl::bom<lexy::utf16_encoding, lexy::encoding_endianness::big>
----

NOTE: There is a UTF-8 BOM, but it is the same regardless of endianness.

NOTE: This rule is only necessary when you have a raw encoding that contains a BOM.
For example, `lexy::read_file()` already handles and deals with BOMs for you by default.

[discrete]
==== `lexy::dsl::encode`

.`lexy/dsl/encode.hpp`
---
encode<Encoding, Endianness>(rule) : Rule
---

The `encode` rule temporarily changes the encoding of the input.
The specified `rule` will be matched using a `Reader` whose encoding is `Encoding` converted from the raw bytes using the specified endianness.
If no `Endianness` is specified, the default is `lexy::encoding_endianness::bom`, and a BOM is matched on the input to determine the endianness.
If no BOM is present, big endian is assumed.

[horizontal]
Requires::
  The input's encoding is a single-byte encoding (usually `lexy::raw_encoding`).
Matches::
  If the endianness is `lexy::encoding_endianness::bom`, matches and consumes an optional BOM to determine endianness.
  Matches and consumes `rule`.
  However, the input of rule are characters according to `Encoding` and `Endianness`, not the single bytes of the actual input.
Values::
  All values produced by the rule.
Errors::
  All errors raised by the rule.
  The error type uses the original reader, not the encoded reader that does the input translation.

[godbolt,cpp,id=Y51r9v]
----
// Matches a UTF-8 code point, followed by an ASCII code point.
dsl::encode<lexy::utf8_encoding>(dsl::code_point)
    + dsl::encode<lexy::ascii_encoding>(dsl::code_point)
----

=== Custom rules

The exact interface for the `Rule`, `Token` and `Branch` concepts is currently still experimental.
Refer to the existing rules if you want to add your own.

